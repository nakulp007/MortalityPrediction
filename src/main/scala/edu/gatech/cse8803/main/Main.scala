package edu.gatech.cse8803.main

import java.text.SimpleDateFormat

import edu.gatech.cse8803.ioutils.{CSVUtils, DataLoader}
import edu.gatech.cse8803.features.FeatureConstruction

import edu.gatech.cse8803.model._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

import java.sql.Date
import java.text.SimpleDateFormat


object Main {
  def main(args: Array[String]) {
    /*  CONFIGURATION  START  */

    //location of MIMIC and other input files
    val dataDir = "data"
    //location of the program generated features file(s)
    val featureDir = "src/main/scala/edu/gatech/cse8803/output_features"
    //location of the output files generated by this
    val outputDir = "src/main/scala/edu/gatech/cse8803/output"
    //location of Prinston stopword file
    val stopWordFileLocation = "princeton_stopwords.txt"

    println("--------------config-----------")
    println(dataDir)
    println(featureDir)
    println(outputDir)
    println(stopWordFileLocation)
    println("--------------config-----------")

    /*  CONFIGURATION  END  */

    import org.apache.log4j.Logger
    import org.apache.log4j.Level

    Logger.getLogger("org").setLevel(Level.WARN)
    Logger.getLogger("akka").setLevel(Level.WARN)

    val sc = createContext
    val sqlContext = new SQLContext(sc)


    /***************** initialize loading of data ********************/
    val (rawPatients, rawDiagnostics, rawMedications, rawLabResults,
      rawNotes, rawComorbidities, rawIcuStays, rawSaps2s) = DataLoader.loadRddRawData(sqlContext, dataDir)

    /***************** Process raw data beforehand! *******************/
    // Process for patients' age. Filters for most recent unique IcuStays for unique patients.
    val (patients, icuStays) = FeatureConstruction.processRawPatientsAndIcuStays(rawPatients, rawIcuStays)
    //patients.foreach(println)
    println(s"Unique patient count: ${patients.count}")
    //icuStays.foreach(println)
    println(s"Unique IcuStay count: ${icuStays.count}")

    // Process notes and get start dates for each patient. Read comments for the function.
    val (notes, firstNoteDates) = FeatureConstruction.processNotesAndCalculateStartDates(
      patients, icuStays, rawNotes)
    println(s"firstNoteDates count: ${firstNoteDates.count}")


    /************************* Generate labels ******************************/
    val labelsInIcu = FeatureConstruction.generateLabelTuples(
      patients, icuStays, FeatureConstruction.InICU())
    //labelsInIcu.foreach(println)
    println(s"${labelsInIcu.count}")

    val labelsIn30Days = FeatureConstruction.generateLabelTuples(
      patients, icuStays, FeatureConstruction.In30Days())
    //labelsIn30Days.foreach(println)
    println(s"${labelsIn30Days.count}")

    val labelsIn1Year = FeatureConstruction.generateLabelTuples(
      patients, icuStays, FeatureConstruction.In1Year())
    //labelsIn1Year.foreach(println)
    println(s"${labelsIn1Year.count}")

    //labelsInIcu.join(labelsIn30Days).join(labelsIn1Year).foreach(println)


    /****************** Baseline Feature Constructions **********************/
    /* Simple baseline feature construction with base features */
    val baseFeatures = FeatureConstruction.constructBaselineFeatureArrayTuples(
      sc, patients, icuStays, rawSaps2s)
    println("------------ Base Features -----------")
    //baseFeatures.foreach(println)
    println(s"Baseline Feature Tuple count: ${baseFeatures.count}")
    println("------------ Base Features End -----------")

    /* Example of adjusting with hours since start time */
    val baseFeatures12 = FeatureConstruction.constructBaselineFeatureArrayTuples(
      sc, patients, icuStays, rawSaps2s, firstNoteDates, 12)
    println("------------ Base Features hr = 12 -----------")
    //baseFeatures.foreach(println)
    println(s"Baseline Feature Tuple at 12 hr count: ${baseFeatures12.count}")
    println("------------ Base Features hr = 12 End -----------")


    sc.stop()
  }

  def createContext(appName: String, masterUrl: String): SparkContext = {
    val conf = new SparkConf().setAppName(appName).setMaster(masterUrl)
    new SparkContext(conf)
  }

  def createContext(appName: String): SparkContext = createContext(appName, "local")

  def createContext: SparkContext = createContext("CSE 8803 Homework Two Application", "local")
}
