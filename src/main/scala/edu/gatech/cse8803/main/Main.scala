package edu.gatech.cse8803.main

import java.text.SimpleDateFormat

import edu.gatech.cse8803.ioutils.{CSVUtils, DataLoader}
import edu.gatech.cse8803.features.FeatureConstruction._

import edu.gatech.cse8803.model._
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

import java.sql.Date
import java.text.SimpleDateFormat


object Main {
  def main(args: Array[String]) {
    /*  CONFIGURATION  START  */

    //location of MIMIC and other input files
    val dataDir = "data"
    //location of the program generated features file(s)
    val featureDir = "src/main/scala/edu/gatech/cse8803/output_features"
    //location of the output files generated by this
    val outputDir = "src/main/scala/edu/gatech/cse8803/output"
    //location of Prinston stopword file
    val stopWordFileLocation = "princeton_stopwords.txt"

    println("--------------config-----------")
    println(dataDir)
    println(featureDir)
    println(outputDir)
    println(stopWordFileLocation)
    println("--------------config-----------")

    /*  CONFIGURATION  END  */

    import org.apache.log4j.Logger
    import org.apache.log4j.Level

    Logger.getLogger("org").setLevel(Level.WARN)
    Logger.getLogger("akka").setLevel(Level.WARN)

    val sc = createContext
    val sqlContext = new SQLContext(sc)


    /***************** initialize loading of data ********************/
    val (rawPatients, rawDiagnostics, rawMedications, rawLabResults,
      rawNotes, rawComorbidities, rawIcuStays, rawSaps2s) = DataLoader.loadRddRawData(sqlContext, dataDir)

    /***************** Process raw data beforehand! *******************/
    // Process for patients' age. Filters for most recent unique IcuStays for unique patients.
    val (patients, icuStays) = processRawPatientsAndIcuStays(rawPatients, rawIcuStays)
    //patients.foreach(println)
    println(s"Unique patient count: ${patients.count}")
    //icuStays.foreach(println)
    println(s"Unique IcuStay count: ${icuStays.count}")

    // Process notes and get start dates for each patient. Read comments for the function.
    val (notes, firstNoteDates) = processNotesAndCalculateStartDates(
      patients, icuStays, rawNotes)
    println(s"firstNoteDates count: ${firstNoteDates.count}")


    /************************* Generate labels ******************************/
    val labelsInIcu = generateLabelTuples(
      patients, icuStays, InICU())
    //labelsInIcu.foreach(println)
    println(s"${labelsInIcu.count}")

    val labelsIn30Days = generateLabelTuples(
      patients, icuStays, In30Days())
    //labelsIn30Days.foreach(println)
    println(s"${labelsIn30Days.count}")

    val labelsIn1Year = generateLabelTuples(
      patients, icuStays, In1Year())
    //labelsIn1Year.foreach(println)
    println(s"${labelsIn1Year.count}")

    //labelsInIcu.join(labelsIn30Days).join(labelsIn1Year).foreach(println)


    /****************** Baseline Feature Constructions **********************/
    /* Example of simple baseline feature construction with base features */
    val baseFeatures = constructBaselineFeatureArrayTuples(
      sc, patients, icuStays, rawSaps2s)
    println("------------ Base Features -----------")
    //baseFeatures.foreach(println)
    println(s"Baseline Feature Tuple count: ${baseFeatures.count}")
    println("------------ Base Features End -----------")


    /**************** Running baseline model based on hours **************/
    val (trainPatients, testPatients) = splitPatientIds(sc, patients, 0.7)

    println(s"${trainPatients.count} train & ${testPatients.count} test patients")

    var go = true
    var hr = 0
    val maxH = 180
    while (hr <= 120 && go) {
      go = runBaseLineModel(sc, trainPatients, testPatients, icuStays, rawSaps2s, firstNoteDates,
        labelsIn30Days, hr)
      hr += 12
    }

    sc.stop()
  }

  val DEFAULT_SEED = 0

  def splitPatientIds(sc: SparkContext, patients: RDD[Patient],
      trainProportion: Double, seed: Long = DEFAULT_SEED): (RDD[Patient], RDD[Patient]) = {
    val splits = patients.randomSplit(Array[Double](trainProportion, 1-trainProportion))
    (splits(0), splits(1))
  }

  def runBaseLineModel(sc: SparkContext, trainPatients: RDD[Patient], testPatients: RDD[Patient],
      icuStays: RDD[IcuStay], saps2s: RDD[Saps2], firstNoteDates: RDD[FirstNoteInfo],
      labels: RDD[LabelTuple], hours: Int): Boolean = {
    val trainTuples = constructBaselineFeatureArrayTuples(sc, trainPatients, icuStays,
      saps2s, firstNoteDates, hours)
    val testTuples = constructBaselineFeatureArrayTuples(sc, testPatients, icuStays,
      saps2s, firstNoteDates, hours)

    if (trainTuples.count == 0) return false

    val trainingPoints = constructForSVM(trainTuples, labels)

    trainingPoints.cache
    val numTraining = trainingPoints.count
    val numTesting = testTuples.count

    val svm = new SVMWithSGD()
    val svmModel = svm.run(trainingPoints)

    svmModel.clearThreshold // Clears threshold so predict() outputs raw prediction scores.

    /* Making predictions */
    val trainPreds = trainPatients.keyBy(_.patientID)
      .join(trainTuples)
      .map{ case(pid, (p, fArr)) => (pid, Vectors.dense(fArr)) }
      .join(labels)
      .map{ case(pid, (vec, label)) => (svmModel.predict(vec), label.toDouble)}

    val testPreds = testPatients.keyBy(_.patientID)
      .join(testTuples)
      .map{ case(pid, (p, fArr)) => (pid, Vectors.dense(fArr)) }
      .join(labels)
      .map{ case(pid, (vec, label)) => (svmModel.predict(vec), label.toDouble)}

    /* Evaluating predictions */
    val trainMetrics = new BinaryClassificationMetrics(trainPreds)
    val testMetrics = new BinaryClassificationMetrics(testPreds)

    val trainAUC = trainMetrics.areaUnderROC
    val testAUC = testMetrics.areaUnderROC
    val prc: RDD[(Double, Double)] = testMetrics.pr // RDD[(recall, precision)]
    println(s"${hours},${numTraining},${numTesting},${trainAUC},${testAUC}")


    true
  }

  def createContext(appName: String, masterUrl: String): SparkContext = {
    val conf = new SparkConf().setAppName(appName).setMaster(masterUrl)
    new SparkContext(conf)
  }

  def createContext(appName: String): SparkContext = createContext(appName, "local")

  def createContext: SparkContext = createContext("CSE 8803 Homework Two Application", "local")
}
