package edu.gatech.cse8803.main

import edu.gatech.cse8803.ioutils.{CSVUtils, DataLoader}
import edu.gatech.cse8803.features.FeatureConstruction._
import edu.gatech.cse8803.main.ModelRunner._

import edu.gatech.cse8803.model._
import org.apache.spark.ml.param.{Param}
import org.apache.spark.ml.tuning.{CrossValidator}
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

import java.sql.Date
import java.text.SimpleDateFormat

object Main {
  def main(args: Array[String]) {
    /*  CONFIGURATION  START  */

    //location of MIMIC and other input files
    val dataDir = "data5000"
    //location of the program generated features file(s)
    val featureDir = "src/main/scala/edu/gatech/cse8803/output_features"
    //location of the output files generated by this
    val outputDir = "src/main/scala/edu/gatech/cse8803/output"
    //location of Prinston stopword file
    val stopWordFileLocation = "princeton_stopwords.txt"

    println("--------------config-----------")
    println(dataDir)
    println(featureDir)
    println(outputDir)
    println(stopWordFileLocation)
    println("--------------config-----------")

    /*  CONFIGURATION  END  */

    import org.apache.log4j.Logger
    import org.apache.log4j.Level

    Logger.getLogger("org").setLevel(Level.WARN)
    Logger.getLogger("akka").setLevel(Level.WARN)

    val sc = createContext
    val sqlContext = new SQLContext(sc)

    val stopwords: Set[String] = sc.textFile(stopWordFileLocation).collect.toSet
    println(s"Stopwords count: ${stopwords.size}")

    /***************** initialize loading of data ********************/
    val (rawPatients, rawDiagnostics, rawMedications, rawLabResults,
      rawNotes, rawComorbidities, rawIcuStays, rawSaps2s) = DataLoader.loadRddRawData(sqlContext, dataDir)

    /***************** Process raw data beforehand! *******************/
    // Process for patients' age. Filters for most recent unique IcuStays for unique patients.
    val (patients1, icuStays1) = processRawPatientsAndIcuStays(rawPatients, rawIcuStays)
    println(s"Unique patient count: ${patients1.count}")
    println(s"Unique IcuStay count: ${icuStays1.count}")

    // Process notes and get start dates for each patient. Read comments for the function.
    val (patients, icuStays, notes, firstNoteDates, tokenizedNotes) = processNotesAndCalculateStartDates(
      patients1, icuStays1, rawNotes, stopwords)

    val patientCount = patients.count
    val useExactSplits = patientCount <= 2000
    println(s"Patients with first note count: ${patientCount}")
    println(s"IcuStays with first note count: ${icuStays.count}")
    println(s"Notes after filtering: ${notes.count}")
    println(s"firstNoteDates count: ${firstNoteDates.count}")


    /************************* Generate labels ******************************/
    val labelsInIcu = generateLabelTuples(patients, icuStays, InICU())
    println(s"Num labels in ICU: ${labelsInIcu.count}")

    val labelsIn30Days = generateLabelTuples(patients, icuStays, In30Days())
    println(s"Num labels in 30 Day: ${labelsIn30Days.count}")

    val labelsIn1Year = generateLabelTuples(patients, icuStays, In1Year())
    println(s"Num labels in 1 Year: ${labelsIn1Year.count}")


    /**************** Running baseline model based on hours **************/
    val (trainPatients, testPatients) = splitPatientIds(sc, patients, 0.7, useExactSplits)
    println(s"${trainPatients.count} train & ${testPatients.count} test patients")

    val labels = labelsInIcu // Change this to change the label type

    // One thing to watch out for from here on out is the fact that labels does not
    // get filtered or subsampled. The RDD is kept exactly the same!
    val subsampledTrainingPatients = subsampleTrainingPatients(
      trainPatients, labels, useExactSplits)
    println(s"${subsampledTrainingPatients.count} subsampled training patients")

    //runBaseLineModel(sc, subsampledTrainingPatients, testPatients,
    //    icuStays, rawSaps2s, firstNoteDates, labels, useCv=false,
    //    numIterations=300,
    //    regParam=100)

    runRetrospectiveTopicModel(sc, subsampledTrainingPatients, testPatients,
        tokenizedNotes, labels,
        numIterations=300,
        regParam=100)

    //runRetrospectiveDerivedFeatureModel(sc, trainPatients, testPatients, labels,
    //    icuStays, rawSaps2s, rawComorbidities,
    //    useCv=false,
    //    numIterations=300,
    //    regParam=100)

    //runTimeVaryingTopicModel(sc, trainPatients, testPatients, labels,
    //    icuStays, firstNoteDates,
    //    tokenizedNotes, numIterations=300, regParam=100)


    sc.stop()
  }

  def splitPatientIds(sc: SparkContext, patients: RDD[Patient],
      trainProportion: Double, exact: Boolean): (RDD[Patient], RDD[Patient]) = {
    if (!exact) {
      val splits = patients.randomSplit(Array[Double](trainProportion, 1-trainProportion))
      (splits(0), splits(1))
    } else {
      val numToSample = (patients.count * 0.7).toInt
      val sample = sc.makeRDD(patients.takeSample(false, numToSample))
      (sample, patients.subtract(sample))
    }
  }

  // Adjusts the ratio of negative to positive examples to be 7:3.
  // Does nothing if the ratio is already smaller than that.
  def subsampleTrainingPatients(patients: RDD[Patient], labels: RDD[LabelTuple], exact: Boolean): RDD[Patient] = {
    val sc = labels.context

    // Count total number of training instances and postivie instances
    val labelsForThesePatients = patients.keyBy(_.patientID).join(labels)
      .map{ case(pid, (p, label)) => (pid, label) }
    val (numTraining, numTrainingPositive) = labelsForThesePatients
      .aggregate((0, 0))(
        (u, labelPair) => (u._1+1, u._2+labelPair._2),
        (u1, u2) => (u1._1+u2._1, u1._2+u2._2)
      )

    println(s"${numTrainingPositive} out of ${numTraining} training examples are positive.")

    // No need to subsample if the ratio is already high
    if (numTrainingPositive / numTraining.toDouble >= 0.3) {
      println("Negative to positive examples proportion is already 7:3 or smaller.")
      return patients
    }

    val positiveLabels = labelsForThesePatients.filter(_._2 == 1)
    val negativeLabels = labelsForThesePatients.filter(_._2 == 0)

    // We want to end up with a 7:3 ratio of negative to positive examples

    // Sample without replacement
    var sample = negativeLabels
    if (!exact) {
      val propToSample = (numTrainingPositive / 0.3 * 0.7) / (numTraining - numTrainingPositive)
      sample = negativeLabels.sample(false, propToSample)
    } else {
      val numToSample = (numTrainingPositive / 0.3 * 0.7).toInt
      sample = sc.makeRDD(negativeLabels.takeSample(false, numToSample))
    }

    // Union the negative with the subsampled positive labels
    val adjustedLabels = sc.union(positiveLabels, sample)

    val adjustedPatients = patients.keyBy(_.patientID).join(adjustedLabels)
      .map{ case((pid, (p, label))) => p }

    adjustedPatients
  }

  def createContext(appName: String, masterUrl: String): SparkContext = {
    val conf = new SparkConf().setAppName(appName).setMaster(masterUrl)
    new SparkContext(conf)
  }

  def createContext(appName: String): SparkContext = createContext(appName, "local")

  def createContext: SparkContext = createContext("CSE6250 Mortality Prediction Application", "local")
}
